{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:22.532064Z",
     "start_time": "2025-02-24T16:00:22.527599Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.143723Z",
     "start_time": "2025-02-24T16:00:22.551191Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('data/final_combined_dataset.csv')",
   "id": "509d60038d86845e",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.173288Z",
     "start_time": "2025-02-24T16:00:23.146239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# geocode_list = [3303807]\n",
    "# df = df[df['geocode'].isin(geocode_list)]"
   ],
   "id": "3c12700c53aa83e5",
   "outputs": [],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.562884Z",
     "start_time": "2025-02-24T16:00:23.174818Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df['week'] = pd.to_numeric(df['week'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Add date_ordinal\n",
    "df['date_ordinal'] = df['date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "df['week'] = df['week'].astype(int) % 100\n",
    "\n",
    "# sum of cases each year\n",
    "cases_sum = df.groupby(['year'])['cases'].sum().reset_index()\n",
    "\n",
    "# Add cyclic month representation\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Add cyclic week representation\n",
    "df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)"
   ],
   "id": "802a8d31e4eb4031",
   "outputs": [],
   "execution_count": 74
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.668227Z",
     "start_time": "2025-02-24T16:00:23.563894Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 2: Create lag features\n",
    "def create_lags(dataframe, group_col, target_col, lags, inplace = False):\n",
    "    if isinstance(target_col, list):  # If target_col is a list of columns\n",
    "        for col in target_col:\n",
    "            for lag in lags:\n",
    "                if inplace:\n",
    "                    dataframe[target_col] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "                else:\n",
    "                    dataframe[f'{col}_lag{lag}'] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "    else:  # If target_col is a single column\n",
    "        for lag in lags:\n",
    "            if inplace:\n",
    "                dataframe[target_col] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "            else:\n",
    "                dataframe[f'{target_col}_lag{lag}'] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "    return dataframe\n",
    "\n",
    "# Lag cases by 1 and 2 weeks\n",
    "data = create_lags(df, group_col='city', target_col='cases', lags=[0, 1])\n",
    "\n",
    "# Lag weather-related variables by 5 and 6 weeks for each city\n",
    "weather_columns = ['tempe_min', 'temp_avg', 'humidity_avg', 'precipitation_avg_ordinary_kriging']\n",
    "data = create_lags(data, group_col='city', target_col=weather_columns, lags=[3, 4])\n",
    "\n",
    "# lag cases -4 weeks\n",
    "data = create_lags(data, group_col='city', target_col='cases', lags=[-2], inplace = True)\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)"
   ],
   "id": "8b4b4e65a1ecf0e8",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.673748Z",
     "start_time": "2025-02-24T16:00:23.669234Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Step 4: Scale continuous variables, including lagged variables\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "continuous_vars = ['tempe_min', 'temp_avg', 'temp_max', 'humidity_max', 'humidity_avg', 'humidity_min', 'vim'\n",
    "                   'precipitation_avg_ordinary_kriging', 'precipitation_max_ordinary_kriging', 'cases',\n",
    "                   'precipitation_avg_regression_kriging', 'precipitation_max_regression_kriging',\n",
    "                   'nearby_cases_weighted']\n",
    "# Include lagged variables in the scaling process\n",
    "lagged_vars = [col for col in data.columns if '_lag' in col]\n",
    "scaler_vars = continuous_vars + lagged_vars"
   ],
   "id": "cb63133631bc3ae4",
   "outputs": [],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.729580Z",
     "start_time": "2025-02-24T16:00:23.674972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 5: Split the data into training and testing sets based on time\n",
    "train_data = data[(data['year'] >= 2012) & (data['year'] <= 2020)]\n",
    "test_data = data[(data['year'] > 2020)]\n",
    "\n",
    "selected_columns = [\n",
    "    'month_sin', 'month_cos', 'week_sin', 'week_cos', 'city', 'week', 'year', 'population',\n",
    "    'cases_lag0', 'cases_lag1',\n",
    "     'temp_avg_lag3', 'humidity_avg_lag3', 'precipitation_avg_ordinary_kriging_lag3', 'temp_avg_lag4', 'humidity_avg_lag4', 'precipitation_avg_ordinary_kriging_lag4', 'vim', 'nearby_cases_weighted'\n",
    "]\n",
    "\n",
    "scaler_vars = list(set(selected_columns) & set(scaler_vars))\n",
    "\n",
    "X_train = train_data[selected_columns]\n",
    "y_train = train_data['cases']\n",
    "\n",
    "X_test = test_data[selected_columns]\n",
    "y_test = test_data['cases']\n",
    "\n",
    "# scale\n",
    "X_train[scaler_vars] = feature_scaler.fit_transform(X_train[scaler_vars])\n",
    "X_test[scaler_vars] = feature_scaler.transform(X_test[scaler_vars])\n",
    "\n",
    "# scale target\n",
    "y_train = target_scaler.fit_transform(y_train.values.reshape(-1, 1))\n",
    "y_test = target_scaler.transform(y_test.values.reshape(-1, 1))"
   ],
   "id": "823cc105023f5a03",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17376\\2187631639.py:20: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train[scaler_vars] = feature_scaler.fit_transform(X_train[scaler_vars])\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17376\\2187631639.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test[scaler_vars] = feature_scaler.transform(X_test[scaler_vars])\n"
     ]
    }
   ],
   "execution_count": 77
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.744104Z",
     "start_time": "2025-02-24T16:00:23.730586Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# \n",
    "# # Ensure you're working with a copy of the DataFrame if necessary\n",
    "# X_train = X_train.copy()  # If it's a slice, we create a new copy\n",
    "# X_test = X_test.copy()    # Same here\n",
    "# \n",
    "# # Step 1: Encode the city column\n",
    "# label_encoder = LabelEncoder()\n",
    "# X_train['city_encoded'] = label_encoder.fit_transform(X_train['city'])\n",
    "# X_test['city_encoded'] = label_encoder.transform(X_test['city'])\n",
    "# \n",
    "# # Step 2: Drop the original city column\n",
    "# X_train = X_train.drop(columns=['city'])\n",
    "# X_test = X_test.drop(columns=['city'])\n",
    "\n",
    "# Convert city column to category dtype\n",
    "X_train['city'] = X_train['city'].astype('category')\n",
    "X_test['city'] = X_test['city'].astype('category')\n"
   ],
   "id": "780f3dc3def8d32a",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17376\\2796289097.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_train['city'] = X_train['city'].astype('category')\n",
      "C:\\Users\\ASUS\\AppData\\Local\\Temp\\ipykernel_17376\\2796289097.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_test['city'] = X_test['city'].astype('category')\n"
     ]
    }
   ],
   "execution_count": 78
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.750622Z",
     "start_time": "2025-02-24T16:00:23.746114Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Step 6: Prepare LightGBM datasets\n",
    "# train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "# test_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)"
   ],
   "id": "882c42c10cc07638",
   "outputs": [],
   "execution_count": 79
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.783681Z",
     "start_time": "2025-02-24T16:00:23.752633Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "\n",
    "def detrend_series(series, period):\n",
    "    stl = STL(series, seasonal=period)\n",
    "    result = stl.fit()\n",
    "    return result.trend, series - result.trend\n",
    "\n",
    "\n",
    "period = y_train.index.freq.n\n",
    "y_train_trend, y_train_residual = detrend_series(y_train, period)\n",
    "y_test_trend, y_test_residual = detrend_series(y_test, period)\n",
    "\n",
    "# Apply STL decomposition to each feature in X\n",
    "X_train_residual = X_train.copy()\n",
    "X_test_residual = X_test.copy()\n",
    "X_train_trend = X_train.copy()\n",
    "X_test_trend = X_test.copy()\n",
    "\n",
    "for col in X_train.columns:\n",
    "    X_train_trend[col], X_train_residual[col] = detrend_series(X_train[col])\n",
    "    X_test_trend[col], X_test_residual[col] = detrend_series(X_test[col])\n",
    "\n",
    "# Create LightGBM datasets with detrended data\n",
    "train_dataset = lgb.Dataset(X_train_residual, label=y_train_residual)\n",
    "test_dataset = lgb.Dataset(X_test_residual, label=y_test_residual)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Train the model\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]\n",
    "model = lgb.train(params, train_dataset, valid_sets=[train_dataset, test_dataset], num_boost_round=1000, callbacks=callbacks)\n",
    "\n",
    "# Predict residuals\n",
    "train_pred_residual = model.predict(X_train_residual)\n",
    "test_pred_residual = model.predict(X_test_residual)\n",
    "\n",
    "# Add back the trend to get final predictions\n",
    "y_train_pred = train_pred_residual + y_train_trend\n",
    "y_test_pred = test_pred_residual + y_test_trend\n",
    "\n",
    "# Evaluate the model\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred))\n",
    "\n",
    "train_r2 = r2_score(y_train, y_train_pred)\n",
    "test_r2 = r2_score(y_test, y_test_pred)\n",
    "\n",
    "print(f'Train RMSE: {train_rmse:.4f}, Train R²: {train_r2:.4f}')\n",
    "print(f'Test RMSE: {test_rmse:.4f}, Test R²: {test_r2:.4f}')"
   ],
   "id": "3c26d370ba16f8b7",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[80], line 12\u001B[0m\n\u001B[0;32m      8\u001B[0m     result \u001B[38;5;241m=\u001B[39m stl\u001B[38;5;241m.\u001B[39mfit()\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m result\u001B[38;5;241m.\u001B[39mtrend, series \u001B[38;5;241m-\u001B[39m result\u001B[38;5;241m.\u001B[39mtrend\n\u001B[1;32m---> 12\u001B[0m period \u001B[38;5;241m=\u001B[39m \u001B[43my_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindex\u001B[49m\u001B[38;5;241m.\u001B[39mfreq\u001B[38;5;241m.\u001B[39mn\n\u001B[0;32m     13\u001B[0m y_train_trend, y_train_residual \u001B[38;5;241m=\u001B[39m detrend_series(y_train, period)\n\u001B[0;32m     14\u001B[0m y_test_trend, y_test_residual \u001B[38;5;241m=\u001B[39m detrend_series(y_test, period)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'numpy.ndarray' object has no attribute 'index'"
     ]
    }
   ],
   "execution_count": 80
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-24T16:00:23.785717Z",
     "start_time": "2025-02-24T16:00:23.785717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importance()\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance = importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print top features in a neat table\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.barh(importance['Feature'], importance['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the highest importance at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4d88594a7f9e627b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "rmse = np.sqrt(mean_squared_error(y_test_actual, y_test_pred))\n",
    "print(f'RMSE: {rmse}')\n",
    "\n",
    "# Step 2: Plot the graph\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(np.arange(len(y_test_actual)), y_test_actual, label='Actual Values', color='blue', alpha=0.7, linewidth=2)\n",
    "plt.plot(np.arange(len(y_test_pred)), y_test_pred, label='Predicted Values', color='yellow', alpha=0.7, linewidth=2)\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Dengue Cases')\n",
    "plt.title('Actual vs Predicted Dengue Cases')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "4f9d356a019f4091",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'year' and 'week' are present in X_test\n",
    "cities = X_test['city'].unique()\n",
    "\n",
    "for city in cities:\n",
    "    # Filter data for the city\n",
    "    city_indices = X_test['city'] == city\n",
    "    y_actual_city = y_test_actual[city_indices]\n",
    "    y_pred_city = y_test_pred[city_indices]\n",
    "\n",
    "    # Combine year and week into 'YYYY-WW' format\n",
    "    weeks_city = X_test.loc[city_indices, 'year'].astype(str) + '-' + X_test.loc[city_indices, 'week'].astype(str)\n",
    "\n",
    "    # Skip the city if the maximum actual dengue cases are less than 10\n",
    "    if np.max(y_actual_city) < 10:\n",
    "        continue\n",
    "\n",
    "    # Compute RMSE for the city\n",
    "    rmse_city = np.sqrt(mean_squared_error(y_actual_city, y_pred_city))\n",
    "    print(f'City: {city}, RMSE: {rmse_city}')\n",
    "\n",
    "    # Plot for the city with weeks on the x-axis\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.plot(weeks_city, y_actual_city, label='Actual Values', color='blue', alpha=0.7, linewidth=2)\n",
    "    plt.plot(weeks_city, y_pred_city, label='Predicted Values', color='orange', alpha=0.7, linewidth=2)\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel('Dengue Cases')\n",
    "    plt.title(f'Actual vs Predicted Dengue Cases for {city}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Select every 4th week for x-axis labels\n",
    "    ticks = weeks_city.iloc[::4]  # Select every 4th week\n",
    "    plt.xticks(ticks, rotation=45)  # Rotate x-axis labels for readability\n",
    "    plt.show()\n"
   ],
   "id": "81132aa9dfd0a866",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
