{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-05-13T19:06:26.392506Z",
     "start_time": "2025-05-13T19:06:07.191244Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Concatenate, Embedding, Flatten, Dropout,Bidirectional\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "2da2dfc0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:06:27.844462Z",
     "start_time": "2025-05-13T19:06:26.396529Z"
    }
   },
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "df = pd.read_csv('final_combined_dataset.csv')\n",
    "\n",
    "# geocode_list = [3303807]\n",
    "# df = df[df['geocode'].isin(geocode_list)]\n",
    "# df = df.drop(columns=['geocode'])\n",
    "\n",
    "df['week'] = pd.to_numeric(df['week'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "# Add date_ordinal\n",
    "df['date_ordinal'] = df['date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Add cyclic month representation\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Add cyclic week representation\n",
    "df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)\n",
    "\n",
    "\n",
    "df[\"year_sin\"] = np.sin(2 * np.pi * df[\"year\"] / df[\"year\"].max())\n",
    "df[\"year_cos\"] = np.cos(2 * np.pi * df[\"year\"] / df[\"year\"].max())\n",
    "\n",
    "\n",
    "# Step 2: Create lag features\n",
    "def create_lags(dataframe, group_col, target_col, lags, inplace = False):\n",
    "    if isinstance(target_col, list):  # If target_col is a list of columns\n",
    "        for col in target_col:\n",
    "            for lag in lags:\n",
    "                if inplace:\n",
    "                    dataframe[target_col] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "                else:\n",
    "                    dataframe[f'{col}_lag{lag}'] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "    else:  # If target_col is a single column\n",
    "        for lag in lags:\n",
    "            if inplace:\n",
    "                dataframe[target_col] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "            else:\n",
    "                dataframe[f'{target_col}_lag{lag}'] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "    return dataframe\n",
    "\n",
    "# Lag cases by 1 and 2 weeks\n",
    "data = create_lags(df, group_col='city', target_col='cases', lags=[0,1, -2])\n",
    "\n",
    "# Lag weather-related variables by 5 and 6 weeks for each city\n",
    "weather_columns = ['tempe_min', 'temp_avg', 'temp_max', 'humidity_max', 'humidity_avg', 'humidity_min',\n",
    "                   'precipitation_avg_ordinary_kriging', 'precipitation_max_ordinary_kriging',\n",
    "                   'precipitation_avg_regression_kriging', 'precipitation_max_regression_kriging']\n",
    "data = create_lags(df, group_col='city', target_col=weather_columns, lags=[3,4])\n",
    "data = data.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Initialize the Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Fit and transform the 'geocode' column\n",
    "data['geocode'] = label_encoder.fit_transform(data['geocode'])\n",
    "\n",
    "train_data = data[data['date'].dt.year <= 2020]\n",
    "test_data = data[data['date'].dt.year >= 2021]\n",
    "\n",
    "print(\"Training set shape:\", train_data.shape)\n",
    "print(\"Test set shape:\", test_data.shape)\n",
    "\n",
    "X_train = train_data.drop(columns=['cases','cases_lag-2'])\n",
    "y_train = train_data[['cases_lag-2']]\n",
    "\n",
    "X_test = test_data.drop(columns=['cases','cases_lag-2'])\n",
    "y_test = test_data[['cases_lag-2']]\n",
    "\n",
    "data = data.drop(columns=['date','cases_per_100k'])\n",
    "\n",
    "\n",
    "columns_not_to_scale = ['week_sin', 'week_cos', 'month_sin', 'month_cos', 'week', 'lat', 'long', \n",
    "                    'geocode', 'year_sin', 'year_cos'] \n",
    "\n",
    "selected_columns = [\n",
    "    'cases_lag0', 'cases_lag1',\n",
    "    'temp_avg', 'humidity_avg',\n",
    "    'precipitation_avg_ordinary_kriging_lag3',\n",
    "    'precipitation_avg_ordinary_kriging_lag4',\n",
    "    'week_sin', 'month_sin', 'week_cos', 'month_cos',\n",
    "    # 'week',\n",
    "    'cases_per_100k',\n",
    "    'vim',\n",
    "    'geocode'\n",
    "]\n",
    "\n",
    "\n",
    "X_train = X_train[selected_columns]\n",
    "X_test = X_test[selected_columns]\n",
    "\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "numeric_cols = [col for col in X_train.columns if X_train[col].dtype in ['float64', 'int64', 'float32', 'int32'] \n",
    "                   and col not in columns_not_to_scale]\n",
    "\n",
    "\n",
    "# Initialize scalers\n",
    "feature_scaler = MinMaxScaler()\n",
    "target_scaler = MinMaxScaler()\n",
    "\n",
    "# Scale the selected numeric features\n",
    "X_train[numeric_cols] = feature_scaler.fit_transform(X_train[numeric_cols])\n",
    "X_test[numeric_cols] = feature_scaler.transform(X_test[numeric_cols])\n",
    "\n",
    "# Scale the target variable\n",
    "y_train = target_scaler.fit_transform(y_train)\n",
    "y_test = target_scaler.transform(y_test)\n",
    "\n",
    "\n",
    "# Filter the dataset to keep only these columns\n",
    "X_train = X_train[selected_columns]\n",
    "X_test = X_test[selected_columns]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape: (42406, 54)\n",
      "Test set shape: (9282, 54)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "92ebb7d9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-13T19:07:32.754593Z",
     "start_time": "2025-05-13T19:06:27.847474Z"
    }
   },
   "source": [
    "seq_length = 4\n",
    "\n",
    "def create_sequences(data, target, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i + seq_length].values)  # Select seq_length rows as input\n",
    "        y.append(target[i + seq_length])  # Select the next row as the target\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "\n",
    "def create_sequences_geocode_wise(data, target, seq_length):\n",
    "    X, y = [], []\n",
    "\n",
    "    # Group by geocode\n",
    "    for geocode in data['geocode'].unique():\n",
    "        geocode_data = data[data['geocode'] == geocode]  # Filter rows for this geocode\n",
    "        geocode_target = target[data['geocode'] == geocode]  # Corresponding target values\n",
    "\n",
    "        # Create sequences for this geocode\n",
    "        X_geo, y_geo = create_sequences(geocode_data, geocode_target, seq_length)\n",
    "\n",
    "        # Append to the lists\n",
    "        X.append(X_geo)\n",
    "        y.append(y_geo)\n",
    "\n",
    "    # Concatenate all geocode-wise sequences\n",
    "    X = np.concatenate(X, axis=0)\n",
    "    y = np.concatenate(y, axis=0)\n",
    "    return X, y\n",
    "\n",
    "\n",
    "# Create sequences for training (geocode-wise)\n",
    "X_train, y_train = create_sequences_geocode_wise(X_train, y_train, seq_length)\n",
    "\n",
    "# Create sequences for testing (geocode-wise)\n",
    "X_test, y_test = create_sequences_geocode_wise(X_test, y_test, seq_length)\n",
    "\n",
    "# Print the shapes to verify\n",
    "print(\"X_train shape:\", X_train.shape)  # (samples, timesteps, features)\n",
    "print(\"y_train shape:\", y_train.shape)  # (samples,)\n",
    "print(\"X_test shape:\", X_test.shape)    # (samples, timesteps, features)\n",
    "print(\"y_test shape:\", y_test.shape)    # (samples,)\n",
    "\n",
    "# Encode geocode as integers\n",
    "df['geocode'] = df['geocode'].astype('category').cat.codes  \n",
    "num_geocodes = df['geocode'].nunique()  # Get number of unique geocodes\n",
    "\n",
    "# Separate geocode after sequence creation\n",
    "X_train_lstm = X_train[:, :, :-1]  # Exclude last column (geocode)\n",
    "X_train_geocode = X_train[:, 0, -1].astype(int)  # Extract geocode as separate categorical input\n",
    "\n",
    "X_test_lstm = X_test[:, :, :-1]\n",
    "X_test_geocode = X_test[:, 0, -1].astype(int)\n",
    "\n",
    "# Print shapes for verification\n",
    "print(\"X_train_lstm shape:\", X_train_lstm.shape)  # (samples, timesteps, features)\n",
    "print(\"X_train_geocode shape:\", X_train_geocode.shape)  # (samples,)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "\n",
    "# Define LSTM model with geocode embeddings\n",
    "seq_length = X_train_lstm.shape[1]  \n",
    "num_features = X_train_lstm.shape[2]  \n",
    "\n",
    "\n",
    "# LSTM Branch\n",
    "# Minimal LSTM model with essential components only\n",
    "\n",
    "# LSTM Branch (Time-Series Data)\n",
    "lstm_input = Input(shape=(seq_length, num_features), name=\"lstm_input\")\n",
    "\n",
    "# First LSTM Layer with Dropout\n",
    "lstm_layer = LSTM(64)(lstm_input)\n",
    "\n",
    "# lstm_layer = LSTM(64, return_sequences=True)(lstm_input)  \n",
    "# dropout_1 = Dropout(0.3)(lstm_layer)  # Drop 30% of neurons\n",
    "\n",
    "# # Second LSTM Layer with Dropout\n",
    "# lstm_layer_2 = LSTM(32)(dropout_1)\n",
    "# dropout_2 = Dropout(0.3)(lstm_layer_2)  # Drop 30% of neurons\n",
    "\n",
    "# Geocode Embedding Branch\n",
    "geocode_input = Input(shape=(1,), name=\"geocode_input\")\n",
    "geocode_embedding = Embedding(input_dim=num_geocodes + 1, output_dim=7)(geocode_input)\n",
    "geocode_embedding = Flatten()(geocode_embedding)\n",
    "\n",
    "# Merge Both Branches\n",
    "merged = Concatenate()([lstm_layer, geocode_embedding])\n",
    "\n",
    "# Output Layer\n",
    "output = Dense(1, activation='linear')(merged)\n",
    "\n",
    "# Build Model\n",
    "model = Model(inputs=[lstm_input, geocode_input], outputs=output)\n",
    "model.compile(\n",
    "    optimizer=Adam(learning_rate=0.001),\n",
    "    loss='mae',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(\n",
    "    monitor='val_loss',\n",
    "    factor=0.5,\n",
    "    patience=5,\n",
    "    min_lr=0.00001\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    [X_train_lstm, X_train_geocode],\n",
    "    y_train,\n",
    "    validation_data=([X_test_lstm, X_test_geocode], y_test),\n",
    "    epochs=100,\n",
    "    batch_size=64,\n",
    "    callbacks=[early_stopping, lr_scheduler]\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_mae = model.evaluate([X_test_lstm, X_test_geocode], y_test)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (42042, 4, 13)\n",
      "y_train shape: (42042, 1)\n",
      "X_test shape: (8918, 4, 13)\n",
      "y_test shape: (8918, 1)\n",
      "X_train_lstm shape: (42042, 4, 12)\n",
      "X_train_geocode shape: (42042,)\n",
      "y_train shape: (42042, 1)\n",
      "Epoch 1/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m8s\u001B[0m 8ms/step - loss: 0.0097 - mae: 0.0097 - val_loss: 0.0019 - val_mae: 0.0019 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 6ms/step - loss: 0.0027 - mae: 0.0027 - val_loss: 9.4654e-04 - val_mae: 9.4654e-04 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 6ms/step - loss: 0.0022 - mae: 0.0022 - val_loss: 0.0024 - val_mae: 0.0024 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 6ms/step - loss: 0.0021 - mae: 0.0021 - val_loss: 0.0015 - val_mae: 0.0015 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m4s\u001B[0m 7ms/step - loss: 0.0017 - mae: 0.0017 - val_loss: 6.1811e-04 - val_mae: 6.1811e-04 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m5s\u001B[0m 8ms/step - loss: 0.0016 - mae: 0.0016 - val_loss: 0.0026 - val_mae: 0.0026 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m6s\u001B[0m 9ms/step - loss: 0.0016 - mae: 0.0016 - val_loss: 7.0644e-04 - val_mae: 7.0644e-04 - learning_rate: 0.0010\n",
      "Epoch 8/100\n",
      "\u001B[1m657/657\u001B[0m \u001B[32m━━━━━━━━━━━━━━━━━━━━\u001B[0m\u001B[37m\u001B[0m \u001B[1m9s\u001B[0m 13ms/step - loss: 0.0014 - mae: 0.0014 - val_loss: 4.8854e-04 - val_mae: 4.8854e-04 - learning_rate: 0.0010\n",
      "Epoch 9/100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 115\u001B[0m\n\u001B[0;32m    102\u001B[0m early_stopping \u001B[38;5;241m=\u001B[39m EarlyStopping(\n\u001B[0;32m    103\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    104\u001B[0m     patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m10\u001B[39m,\n\u001B[0;32m    105\u001B[0m     restore_best_weights\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    106\u001B[0m )\n\u001B[0;32m    108\u001B[0m lr_scheduler \u001B[38;5;241m=\u001B[39m ReduceLROnPlateau(\n\u001B[0;32m    109\u001B[0m     monitor\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mval_loss\u001B[39m\u001B[38;5;124m'\u001B[39m,\n\u001B[0;32m    110\u001B[0m     factor\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.5\u001B[39m,\n\u001B[0;32m    111\u001B[0m     patience\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m5\u001B[39m,\n\u001B[0;32m    112\u001B[0m     min_lr\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.00001\u001B[39m\n\u001B[0;32m    113\u001B[0m )\n\u001B[1;32m--> 115\u001B[0m history \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    116\u001B[0m \u001B[43m    \u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_train_lstm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_train_geocode\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    117\u001B[0m \u001B[43m    \u001B[49m\u001B[43my_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalidation_data\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43mX_test_lstm\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX_test_geocode\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_test\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    119\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    120\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m64\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    121\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m[\u001B[49m\u001B[43mearly_stopping\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlr_scheduler\u001B[49m\u001B[43m]\u001B[49m\n\u001B[0;32m    122\u001B[0m \u001B[43m)\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[38;5;66;03m# Evaluate\u001B[39;00m\n\u001B[0;32m    125\u001B[0m test_loss, test_mae \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mevaluate([X_test_lstm, X_test_geocode], y_test)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:117\u001B[0m, in \u001B[0;36mfilter_traceback.<locals>.error_handler\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    115\u001B[0m filtered_tb \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m    116\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    118\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    119\u001B[0m     filtered_tb \u001B[38;5;241m=\u001B[39m _process_traceback_frames(e\u001B[38;5;241m.\u001B[39m__traceback__)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\trainer.py:370\u001B[0m, in \u001B[0;36mTensorFlowTrainer.fit\u001B[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001B[0m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m epoch_iterator\u001B[38;5;241m.\u001B[39mcatch_stop_iteration():\n\u001B[0;32m    369\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m step, iterator \u001B[38;5;129;01min\u001B[39;00m epoch_iterator:\n\u001B[1;32m--> 370\u001B[0m         \u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mon_train_batch_begin\u001B[49m\u001B[43m(\u001B[49m\u001B[43mstep\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    371\u001B[0m         logs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrain_function(iterator)\n\u001B[0;32m    372\u001B[0m         callbacks\u001B[38;5;241m.\u001B[39mon_train_batch_end(step, logs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\keras\\src\\callbacks\\callback_list.py:147\u001B[0m, in \u001B[0;36mCallbackList.on_train_batch_begin\u001B[1;34m(self, batch, logs)\u001B[0m\n\u001B[0;32m    144\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n\u001B[0;32m    145\u001B[0m         callback\u001B[38;5;241m.\u001B[39mon_epoch_end(epoch, logs)\n\u001B[1;32m--> 147\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mon_train_batch_begin\u001B[39m(\u001B[38;5;28mself\u001B[39m, batch, logs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    148\u001B[0m     logs \u001B[38;5;241m=\u001B[39m python_utils\u001B[38;5;241m.\u001B[39mpythonify_logs(logs)\n\u001B[0;32m    149\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m callback \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcallbacks:\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "45bddf48",
   "metadata": {},
   "source": [
    "# Make predictions on the test data\n",
    "\n",
    "\n",
    "predictions = model.predict([X_test_lstm, X_test_geocode])\n",
    "\n",
    "# Rescale actual values back to the original scale\n",
    "y_test_rescaled = target_scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "# Rescale the predicted values back to the original scale\n",
    "test_predictions_rescaled = target_scaler.inverse_transform(predictions)\n",
    "test_predictions_rescaled = np.clip(test_predictions_rescaled, 0, None)\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Predictions for training set\n",
    "train_predictions = model.predict([X_train_lstm, X_train_geocode])\n",
    "y_train_rescaled = target_scaler.inverse_transform(y_train.reshape(-1, 1))\n",
    "train_predictions_rescaled = target_scaler.inverse_transform(train_predictions)\n",
    "train_predictions_rescaled = np.clip(train_predictions_rescaled, 0, None)\n",
    "\n",
    "# R² for train set\n",
    "r2_train = r2_score(y_train_rescaled, train_predictions_rescaled)\n",
    "\n",
    "# R² for test set\n",
    "r2_test = r2_score(y_test_rescaled, test_predictions_rescaled)\n",
    "\n",
    "train_rmse = np.sqrt(mean_squared_error(y_train, y_train_rescaled))\n",
    "test_rmse = np.sqrt(mean_squared_error(y_test, test_predictions_rescaled))\n",
    "\n",
    "# Print metrics\n",
    "print(f\"Train R²: {r2_train:.4f}\")\n",
    "print(f\"Test R²: {r2_test:.4f}\")\n",
    "print(f\"Test MAE: {test_mae:.4f}\")  # Already printed before, but reprinted here for completeness\n",
    "print(\"Train RMSE:\", train_rmse)\n",
    "print(\"Test RMSE:\", test_rmse)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b6a2d060",
   "metadata": {},
   "source": [
    "# Step 1: Prepare test_df\n",
    "test_df = test_data.copy().reset_index(drop=True)\n",
    "test_df[numeric_cols] = feature_scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "\n",
    "# Step 2: Drop the first `seq_length` rows *per geocode*\n",
    "filtered_test_df_list = []\n",
    "\n",
    "for geocode in test_df['geocode'].unique():\n",
    "    geo_df = test_df[test_df['geocode'] == geocode]\n",
    "    if len(geo_df) > seq_length:\n",
    "        geo_df = geo_df.iloc[seq_length:]  # drop first few rows\n",
    "        filtered_test_df_list.append(geo_df)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Concatenate and reset index\n",
    "test_df_seq = pd.concat(filtered_test_df_list).reset_index(drop=True)\n",
    "\n",
    "test_df_seq['lstm_pred'] = test_predictions_rescaled.flatten()\n",
    "test_df_seq['actual'] = y_test_rescaled.flatten()\n",
    "# Step 4: Decode geocode if label encoded\n",
    "test_df_seq['geocode'] = label_encoder.inverse_transform(test_df_seq['geocode'].astype(int))\n",
    "\n",
    "# Step 5: Add year column\n",
    "test_df_seq['year'] = pd.to_datetime(test_df_seq['date']).dt.year\n",
    "\n",
    "# Step 6: Export year-wise predictions\n",
    "test_df_seq[test_df_seq['year'] == 2021][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_preds_2021.csv', index=False)\n",
    "test_df_seq[test_df_seq['year'] == 2022][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_test_preds_2022.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "a27e1b96",
   "metadata": {},
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# # Create a DataFrame for actual values\n",
    "# y_test_df = pd.DataFrame(y_test_rescaled.flatten(), columns=['actual'])\n",
    "\n",
    "# # Create a DataFrame for predicted values\n",
    "# y_pred_df = pd.DataFrame(test_predictions_rescaled.flatten(), columns=['lstm_pred'])\n",
    "\n",
    "# # Ensure test_df_seq is trimmed to the right number of rows\n",
    "# test_df_seq = test_df_seq.iloc[:len(y_test_df)].reset_index(drop=True)\n",
    "# pd.set_option('display.max_rows', None)\n",
    "\n",
    "# # Add both actual and predicted values\n",
    "# test_df_seq['actual'] = y_test_df['actual']\n",
    "# test_df_seq['lstm_pred'] = y_pred_df['lstm_pred']\n",
    "# test_df_seq\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1eab94d3",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41dd1f67",
   "metadata": {},
   "source": [
    "# Step 1: Prepare test_df\n",
    "test_df = test_data.copy().reset_index(drop=True)\n",
    "test_df[numeric_cols] = feature_scaler.transform(test_df[numeric_cols])\n",
    "\n",
    "\n",
    "# Step 2: Drop the first `seq_length` rows *per geocode*\n",
    "filtered_test_df_list = []\n",
    "\n",
    "for geocode in test_df['geocode'].unique():\n",
    "    geo_df = test_df[test_df['geocode'] == geocode]\n",
    "    if len(geo_df) > seq_length:\n",
    "        geo_df = geo_df.iloc[seq_length:]  # drop first few rows\n",
    "        filtered_test_df_list.append(geo_df)\n",
    "\n",
    "\n",
    "\n",
    "# Step 3: Concatenate and reset index\n",
    "test_df_seq = pd.concat(filtered_test_df_list).reset_index(drop=True)\n",
    "\n",
    "test_df_seq['lstm_pred'] = test_predictions_rescaled.flatten()\n",
    "test_df_seq['actual'] = y_test_rescaled.flatten()\n",
    "# Step 4: Decode geocode if label encoded\n",
    "test_df_seq['geocode'] = label_encoder.inverse_transform(test_df_seq['geocode'].astype(int))\n",
    "\n",
    "# Step 5: Add year column\n",
    "test_df_seq['year'] = pd.to_datetime(test_df_seq['date']).dt.year\n",
    "\n",
    "# Step 6: Export year-wise predictions\n",
    "test_df_seq[test_df_seq['year'] == 2021][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_preds_2021.csv', index=False)\n",
    "test_df_seq[test_df_seq['year'] == 2022][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_test_preds_2022.csv', index=False)\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "959cf91f",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Limit to a few geocodes if you have many\n",
    "geocodes_to_plot = test_df_seq['geocode'].unique()[:5]  # Change or remove this line to plot all\n",
    "\n",
    "for geocode in geocodes_to_plot:\n",
    "    geo_data = test_df_seq[test_df_seq['geocode'] == geocode]\n",
    "    \n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(geo_data['date'], geo_data['actual'], label='Actual', marker='o')\n",
    "    plt.plot(geo_data['date'], geo_data['lstm_pred'], label='Predicted', marker='x')\n",
    "    \n",
    "    plt.title(f'Actual vs Predicted for Geocode {geocode}')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Cases')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "756895b4",
   "metadata": {},
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Number of cases per plot (as you specified: 100)\n",
    "chunk_size = 100\n",
    "num_chunks = len(test_predictions_rescaled) // chunk_size\n",
    "\n",
    "# Loop through each chunk and plot the actual vs predicted values\n",
    "for i in range(num_chunks):\n",
    "    # Get the start and end indices for the current chunk\n",
    "    start_idx = i * chunk_size\n",
    "    end_idx = (i + 1) * chunk_size\n",
    "    \n",
    "    # Get the actual and predicted values for the current chunk\n",
    "    actual_chunk = y_test_rescaled[start_idx:end_idx]\n",
    "    predicted_chunk = test_predictions_rescaled[start_idx:end_idx]\n",
    "    \n",
    "    # Plot the actual vs predicted values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(actual_chunk, label='Actual', color='blue')\n",
    "    plt.plot(predicted_chunk, label='Predicted', color='red', linestyle='--')\n",
    "    plt.title(f'Actual vs Predicted for Chunk {i+1}')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Values')\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "62634346",
   "metadata": {},
   "source": [
    "# Make a copy of test_data\n",
    "test_data_for_preds = test_data.copy().reset_index(drop=True)\n",
    "\n",
    "# Preserve geocode and date for tracking\n",
    "date_series = test_data_for_preds['date']\n",
    "geocode_series = test_data_for_preds['geocode']\n",
    "\n",
    "# Prepare filtered X_test (model input only)\n",
    "filtered_X_test = test_data_for_preds[selected_columns].copy()\n",
    "filtered_X_test[numeric_cols] = feature_scaler.transform(filtered_X_test[numeric_cols])\n",
    "\n",
    "# Add back geocode and date (used only for labeling, not for model)\n",
    "filtered_X_test['date'] = date_series\n",
    "filtered_X_test['geocode'] = geocode_series\n",
    "\n",
    "# Reset index to sync with test_predictions_rescaled\n",
    "filtered_X_test = filtered_X_test.reset_index(drop=True)\n",
    "\n",
    "# Initialize\n",
    "seq_geocode = []\n",
    "seq_year = []\n",
    "seq_actual = []\n",
    "seq_preds = []\n",
    "\n",
    "# Loop through each geocode\n",
    "for geocode in filtered_X_test['geocode'].unique():\n",
    "    geo_data = filtered_X_test[filtered_X_test['geocode'] == geocode].reset_index(drop=True)\n",
    "\n",
    "    if len(geo_data) < seq_length + 1:\n",
    "        continue\n",
    "\n",
    "    for i in range(len(geo_data) - seq_length):\n",
    "        pos = i + seq_length\n",
    "        global_pos = geo_data.index[pos]\n",
    "\n",
    "        # Get data\n",
    "        date_val = geo_data.iloc[pos]['date']\n",
    "        y_actual = y_test_rescaled[global_pos][i]\n",
    "        pred = test_predictions_rescaled[global_pos][0]\n",
    "        true_geocode = geo_data.iloc[pos]['geocode']\n",
    "\n",
    "        # Store\n",
    "        seq_geocode.append(true_geocode)\n",
    "        seq_year.append(date_val.year)\n",
    "        seq_actual.append(y_actual)\n",
    "        seq_preds.append(pred)\n",
    "\n",
    "# Create DataFrame\n",
    "result_df = pd.DataFrame({\n",
    "    'geocode': seq_geocode,\n",
    "    'year': seq_year,\n",
    "    'lstm_pred': seq_preds,\n",
    "    'actual': seq_actual\n",
    "})\n",
    "\n",
    "# Map back from encoded geocode to original\n",
    "result_df['geocode'] = label_encoder.inverse_transform(result_df['geocode'].astype(int))\n",
    "\n",
    "\n",
    "# Sort properly\n",
    "result_df = result_df.sort_values(by=['geocode', 'year'])\n",
    "\n",
    "# Export\n",
    "result_df[result_df['year'] == 2021][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_preds_2021.csv', index=False)\n",
    "result_df[result_df['year'] == 2022][['geocode', 'lstm_pred', 'actual']].to_csv('lstm_test_preds_2022.csv', index=False)\n",
    "\n",
    "print(\"Prediction files saved: lstm_preds.csv and lstm_test_preds.csv\")\n"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
