{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-07-10T10:07:27.868363Z",
     "start_time": "2025-07-10T10:07:23.054697Z"
    }
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import lightgbm as lgb"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpandas\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mpd\u001B[39;00m\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmetrics\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m mean_squared_error\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpreprocessing\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m StandardScaler\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mlightgbm\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mlgb\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\__init__.py:143\u001B[0m\n\u001B[0;32m    141\u001B[0m _BUILT_WITH_MESON \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 143\u001B[0m     \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msklearn\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_built_with_meson\u001B[39;00m  \u001B[38;5;66;03m# noqa: F401\u001B[39;00m\n\u001B[0;32m    145\u001B[0m     _BUILT_WITH_MESON \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mModuleNotFoundError\u001B[39;00m:\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1360\u001B[0m, in \u001B[0;36m_find_and_load\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:1331\u001B[0m, in \u001B[0;36m_find_and_load_unlocked\u001B[1;34m(name, import_)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap>:935\u001B[0m, in \u001B[0;36m_load_unlocked\u001B[1;34m(spec)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:991\u001B[0m, in \u001B[0;36mexec_module\u001B[1;34m(self, module)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1087\u001B[0m, in \u001B[0;36mget_code\u001B[1;34m(self, fullname)\u001B[0m\n",
      "File \u001B[1;32m<frozen importlib._bootstrap_external>:1186\u001B[0m, in \u001B[0;36mget_data\u001B[1;34m(self, path)\u001B[0m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "df = pd.read_csv('data/final_combined_dataset.csv')",
   "id": "509d60038d86845e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# geocode_list = [3303807]\n",
    "# df = df[df['geocode'].isin(geocode_list)]"
   ],
   "id": "3c12700c53aa83e5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "df['week'] = pd.to_numeric(df['week'], errors='coerce')\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Add date_ordinal\n",
    "df['date_ordinal'] = df['date'].apply(lambda x: x.toordinal())\n",
    "\n",
    "# Extract year and month\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month'] = df['date'].dt.month\n",
    "\n",
    "# Add cyclic month representation\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Add cyclic week representation\n",
    "df['week_sin'] = np.sin(2 * np.pi * df['week'] / 52)\n",
    "df['week_cos'] = np.cos(2 * np.pi * df['week'] / 52)"
   ],
   "id": "e6e68d313100e20b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Define outbreak categories based on cases_per_100k thresholds\n",
    "bins = [0, 100, 200, 250, 400, float('inf')]\n",
    "labels = ['No', 'Mild', 'Moderate', 'High', 'Severe']\n",
    "\n",
    "# Create the 'outbreak' column\n",
    "df['outbreak'] = pd.cut(df['cases_per_100k'], bins=bins, labels=labels, right=False)"
   ],
   "id": "2f3c79110987b17a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 2: Create lag features\n",
    "def create_lags(dataframe, group_col, target_col, lags, inplace = False):\n",
    "    if isinstance(target_col, list):  # If target_col is a list of columns\n",
    "        for col in target_col:\n",
    "            for lag in lags:\n",
    "                if inplace:\n",
    "                    dataframe[target_col] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "                else:\n",
    "                    dataframe[f'{col}_lag{lag}'] = dataframe.groupby(group_col)[col].shift(lag)\n",
    "    else:  # If target_col is a single column\n",
    "        for lag in lags:\n",
    "            if inplace:\n",
    "                dataframe[target_col] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "            else:\n",
    "                dataframe[f'{target_col}_lag{lag}'] = dataframe.groupby(group_col)[target_col].shift(lag)\n",
    "    return dataframe\n",
    "\n",
    "# Lag cases by 1 and 2 weeks\n",
    "data = create_lags(df, group_col='city', target_col='cases', lags=[0, 1])\n",
    "\n",
    "# Lag cases by 1 and 2 weeks\n",
    "data = create_lags(df, group_col='city', target_col='cases_per_100k', lags=[0, 1])\n",
    "\n",
    "# Lag weather-related variables by 5 and 6 weeks for each city\n",
    "weather_columns = ['tempe_min', 'temp_avg', 'temp_max', 'humidity_max', 'humidity_avg', 'humidity_min',\n",
    "                   'precipitation_avg_ordinary_kriging', 'precipitation_max_ordinary_kriging',\n",
    "                   'precipitation_avg_regression_kriging', 'precipitation_max_regression_kriging']\n",
    "data = create_lags(data, group_col='city', target_col=weather_columns, lags=[0, 1, 2, 3, 4])\n",
    "\n",
    "# data = create_lags(data, group_col='city', target_col='nearby_cases_weighted', lags=[4])\n",
    "\n",
    "# lag cases -4 weeks\n",
    "data = create_lags(data, group_col='city', target_col='cases_per_100k', lags=[-2], inplace = True)\n",
    "\n",
    "data = data.dropna().reset_index(drop=True)"
   ],
   "id": "4d39d5f811156b64"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 4: Scale continuous variables, including lagged variables\n",
    "feature_scaler = StandardScaler()\n",
    "target_scaler = StandardScaler()\n",
    "\n",
    "continuous_vars = ['tempe_min', 'temp_avg', 'temp_max', 'humidity_max', 'humidity_avg', 'humidity_min',\n",
    "                   'precipitation_avg_ordinary_kriging', 'precipitation_max_ordinary_kriging',\n",
    "                   'precipitation_avg_regression_kriging', 'precipitation_max_regression_kriging',\n",
    "                'nearby_cases_weighted']\n",
    "# Include lagged variables in the scaling process\n",
    "lagged_vars = [col for col in data.columns if '_lag' in col]\n",
    "scaler_vars = continuous_vars + lagged_vars\n",
    "# data[scaler_vars] = feature_scaler.fit_transform(data[scaler_vars])\n",
    "# \n",
    "# # Scale the target variable\n",
    "# scaled_cases = target_scaler.fit_transform(data[['cases_per_100k']])\n",
    "# data['scaled_cases'] = scaled_cases"
   ],
   "id": "4f14de0da79efa63"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 5: Split the data into training and testing sets based on time\n",
    "train_data = data[(data['week'] >= 201201) & (data['week'] <= 202052)]\n",
    "test_data = data[(data['week'] > 202052)]\n",
    "\n",
    "columns_exclude = ['date', 'cases', 'outbreak', 'scaled_cases', 'lat', 'long', 'population', 'cases_per_100k', 'year', 'month']\n",
    "\n",
    "# Prepare X (features)\n",
    "X_train = train_data[[col for col in data.columns if col not in columns_exclude]]\n",
    "X_test = test_data[[col for col in data.columns if col not in columns_exclude]]\n",
    "\n",
    "# Prepare y (target variable)\n",
    "y_train = train_data['outbreak']\n",
    "y_test = test_data['outbreak']\n",
    "\n",
    "# Convert categorical target variable to numerical labels\n",
    "y_train = y_train.astype('category').cat.codes  # No=0, Mild=1, Moderate=2, High=3\n",
    "y_test = y_test.astype('category').cat.codes\n",
    "\n",
    "# Scale features\n",
    "X_train[scaler_vars] = feature_scaler.fit_transform(X_train[scaler_vars])\n",
    "X_test[scaler_vars] = feature_scaler.transform(X_test[scaler_vars])\n"
   ],
   "id": "ab69ee587c646e79"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import json\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "\n",
    "# Ensure you're working with a copy of the DataFrame\n",
    "X_train = X_train.copy()\n",
    "X_test = X_test.copy()\n",
    "\n",
    "# Step 1: Encode the city column\n",
    "label_encoder = LabelEncoder()\n",
    "X_train['city_encoded'] = label_encoder.fit_transform(X_train['city'])\n",
    "X_test['city_encoded'] = label_encoder.transform(X_test['city'])\n",
    "\n",
    "# Convert encoding to a dictionary with Python int values\n",
    "city_mapping = {city: int(encoded) for city, encoded in zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))}\n",
    "\n",
    "# Save the mapping as JSON with geocode as key\n",
    "geocode_mapping = {city_mapping[city]: geocode for geocode, city in zip(X_train['geocode'], X_train['city'])}\n",
    "with open('city_mapping.json', 'w') as f:\n",
    "    json.dump(geocode_mapping, f, indent=4)\n",
    "\n",
    "# Step 2: Drop the original city column\n",
    "X_train = X_train.drop(columns=['city', 'geocode'])\n",
    "X_test = X_test.drop(columns=['city', 'geocode'])"
   ],
   "id": "c86d8c306cdd798"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Step 6: Prepare LightGBM datasets\n",
    "train_dataset = lgb.Dataset(X_train, label=y_train)\n",
    "test_dataset = lgb.Dataset(X_test, label=y_test, reference=train_dataset)"
   ],
   "id": "13864b5f2680f167"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Step 7: Define LightGBM parameters for classification\n",
    "params = {\n",
    "    'objective': 'multiclass',\n",
    "    'metric': 'multi_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'num_class': 5,  # Four outbreak categories\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "# Step 8: Train the model with early stopping\n",
    "callbacks = [lgb.early_stopping(stopping_rounds=50), lgb.log_evaluation(period=100)]\n",
    "model = lgb.train(params, train_dataset, valid_sets=[train_dataset, test_dataset],\n",
    "                  num_boost_round=1000, callbacks=callbacks)\n",
    "\n",
    "# Step 9: Make predictions\n",
    "probs = model.predict(X_test, num_iteration=model.best_iteration)  # Probabilities for each class\n",
    "y_pred = np.argmax(probs, axis=1) # Convert probabilities to class labels\n",
    "\n",
    "# Step 10: Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "print(classification_report(y_test, y_pred, target_names=['No', 'Mild', 'Moderate', 'High', 'Severe']))\n",
    "\n",
    "# Step 11: Save the model\n",
    "model.save_model('lightgbm_dengue_model.txt')\n"
   ],
   "id": "b40e2e8a15a75bd5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a DataFrame for feature importance\n",
    "importance = pd.DataFrame({\n",
    "    'Feature': X_train.columns,\n",
    "    'Importance': model.feature_importance()\n",
    "})\n",
    "\n",
    "# Sort by importance\n",
    "importance = importance.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Print top features in a neat table\n",
    "print(\"Top 10 Most Important Features:\")\n",
    "print(importance.head(10))\n",
    "\n",
    "# Visualize feature importance\n",
    "plt.figure(figsize=(10, 14))\n",
    "plt.barh(importance['Feature'], importance['Importance'], color='skyblue')\n",
    "plt.xlabel('Importance Score')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Feature Importance')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to show the highest importance at the top\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "c27d500f0d6e5465"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: Evaluate the classification performance\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=['No', 'Mild', 'Moderate', 'High', 'Severe']))\n",
    "\n",
    "# Step 2: Confusion Matrix\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(conf_matrix)\n",
    "\n",
    "# Step 3: Plot Actual vs Predicted Labels\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.scatter(np.arange(len(y_test)), y_test, label='Actual Labels', color='blue', alpha=0.7, marker='o')\n",
    "plt.scatter(np.arange(len(y_pred)), y_pred, label='Predicted Labels', color='orange', alpha=0.7, marker='x')\n",
    "\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Outbreak Level (0=No, 1=Mild, 2=Moderate, 3=High, 4=Severe)')\n",
    "plt.title('Actual vs Predicted Outbreak Levels')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "c0df44d0887dff2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'city_encoded' and 'week' are present in X_test\n",
    "cities = X_test['city_encoded'].unique()\n",
    "\n",
    "for city in cities:\n",
    "    # Filter data for the city\n",
    "    city_indices = X_test['city_encoded'] == city\n",
    "    y_actual_city = y_test[city_indices]\n",
    "    y_pred_city = y_pred[city_indices]\n",
    "    weeks_city = X_test['week'][city_indices].astype(str)\n",
    "\n",
    "    # Skip the city if the maximum actual outbreak level is less than 1 (i.e., No outbreak)\n",
    "    if np.max(y_actual_city) < 1:\n",
    "        continue\n",
    "\n",
    "    # Compute accuracy for the city\n",
    "    accuracy_city = accuracy_score(y_actual_city, y_pred_city)\n",
    "    print(f'City: {city}, Accuracy: {accuracy_city:.4f}')\n",
    "\n",
    "    # Check unique classes in actual and predicted values\n",
    "    unique_classes = np.unique(y_actual_city)\n",
    "    print(f'Unique classes for {city}: {unique_classes}')\n",
    "\n",
    "    # Define target names based on unique classes\n",
    "    target_names = ['No', 'Mild', 'Moderate', 'High', 'Severe']\n",
    "    if len(unique_classes) < len(target_names):\n",
    "        target_names = target_names[:len(unique_classes)]\n",
    "\n",
    "    # Classification report\n",
    "    print(f\"Classification Report for {city}:\")\n",
    "    print(classification_report(y_actual_city, y_pred_city, target_names=target_names))\n",
    "\n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_actual_city, y_pred_city)\n",
    "    print(f\"Confusion Matrix for {city}:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Plot for the city with weeks on the x-axis\n",
    "    plt.figure(figsize=(16, 8))\n",
    "    plt.scatter(weeks_city, y_actual_city, label='Actual Labels', color='blue', alpha=0.7, marker='o')\n",
    "    plt.scatter(weeks_city, y_pred_city, label='Predicted Labels', color='orange', alpha=0.7, marker='x')\n",
    "    plt.xlabel('Week')\n",
    "    plt.ylabel('Outbreak Level (0=No, 1=Mild, 2=Moderate, 3=High, 4=Severe)')\n",
    "    plt.title(f'Actual vs Predicted Outbreak Levels for {city}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Select every 4th week for x-axis labels\n",
    "    ticks = weeks_city[::4]  # Select every 4th week\n",
    "    plt.xticks(ticks, rotation=45)  # Rotate x-axis labels for readability\n",
    "    plt.show()\n"
   ],
   "id": "d26a733937197796"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Save the xtest and ytest together\n",
    "X_test['outbreak'] = y_test\n",
    "X_test.to_csv('data/xtest_ytest.csv', index=False)"
   ],
   "id": "a2d1095dfbdc3e1c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "f4a1fe4bf282060"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
